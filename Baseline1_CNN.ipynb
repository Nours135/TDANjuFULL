{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先从简单的CNN开始写吧，嘶，主要是数据太少了orzzzzz\n",
    "# 所以只能做分类了，完蛋，我开始写之前怎么就没想到这点呢\n",
    "# 没事，3个类，33个数据一类，勉勉强强够了，大不了我自己再洗一年的数据\n",
    "\n",
    "import pandas as pd\n",
    "from utils import randomSplit, Accumulator\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold         # k折交叉验\n",
    "from sklearn.preprocessing import StandardScaler  # 归一化\n",
    "\n",
    "\n",
    "def rank2class(df):\n",
    "    if df['LgRk'] <= 6:\n",
    "        return 0\n",
    "    elif 6 < df['LgRk'] <= 13:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 30, 151])\n"
     ]
    }
   ],
   "source": [
    "class myDataset():\n",
    "    def __init__(self):\n",
    "        self.player = pd.read_csv('DLdata\\outfielder_combined_from_raw_fillna.csv', header=0, encoding='utf-8')\n",
    "        self.player = self.player.drop(['Player', 'Pos', 'Comp', 'Age', 'Born'], axis=1)\n",
    "        scaler = StandardScaler()  # z score 归一化\n",
    "        for col in self.player.columns:\n",
    "            if col in ('Squad', \"Season\"): continue\n",
    "            self.player[[col]] = scaler.fit_transform(self.player[[col]])\n",
    "\n",
    "\n",
    "        self.league = pd.read_excel('DLdata/SquadPerformance2021.xlsx', sheet_name='Sheet1')\n",
    "        self.league['tier'] = self.league.apply(rank2class, axis=1)\n",
    "        self.league = self.league[['Squad', 'Season', 'tier']]  \n",
    "        \n",
    "        #self.size_stat = [] 结果是这个最大的数据集，是30\n",
    "\n",
    "        # 生成连续的变量\n",
    "        self.X = []  # in GNN: list of Data object; in CNN: list of 2-dim tensors; in MLs list of 1-dim tensors\n",
    "        self.y = []\n",
    "        for id in self.league.index:\n",
    "            squad = self.league.at[id, 'Squad']\n",
    "            season = self.league.at[id, 'Season']\n",
    "            if season == 2021: continue  # 发现github的数据在2021年仅仅只有前几场比赛的，所以不得不舍弃了orzzz\n",
    "            self.X.append(self.Squad2Player(squad, season)) \n",
    "            self.y.append([self.league.at[id, 'tier']])  # 作为监督的label\n",
    "\n",
    "\n",
    "    def Squad2Player(self, squad, season):\n",
    "        \"输入squad的名字，输出球队球员的数据\"\n",
    "        season = str(season) + '-' + str(int(season) + 1)\n",
    "        re = self.player[self.player['Squad'] == squad]\n",
    "        #print(re.columns)\n",
    "        re = re[re['Season'] == season].drop(['Squad', 'Season'], axis=1)\n",
    "        re = torch.Tensor(re.values)  # shape [nplayers, features]\n",
    "        # print(re.shape) [20, 151]\n",
    "        #self.size_stat.append(re.shape[0])\n",
    "\n",
    "        upBound = 30\n",
    "        if re.shape[0] < upBound:\n",
    "            a = (upBound - re.shape[0]) // 2\n",
    "            b = upBound - re.shape[0] - a\n",
    "            a = torch.zeros((a, re.shape[1]))\n",
    "            b = torch.zeros((b, re.shape[1]))\n",
    "            return torch.cat([a, re, b], dim=0).unsqueeze(0)\n",
    "        elif re.shape[0] == upBound:\n",
    "            return re.unsqueeze(0)\n",
    "        else:\n",
    "            return re[:upBound, :].unsqueeze(0)\n",
    "        \n",
    "    def KFolder(self, K=10, shuffle=True):\n",
    "            KF = KFold(n_splits=K, shuffle=shuffle)  \n",
    "            for train_index, test_index in KF.split(self.X):\n",
    "                yield train_index, test_index\n",
    "    \n",
    "    def DataIter(self, batchsize, data_index):\n",
    "        '''输入K折交叉验证给出来的index'''\n",
    "        \n",
    "        for i in range(0, data_index.shape[0], batchsize):\n",
    "            upper = min(i + batchsize, data_index.shape[0])\n",
    "            x_l = []\n",
    "            y_l = []\n",
    "            for j in range(i, upper):\n",
    "                x_l.append(self.X[j].unsqueeze(0))\n",
    "                y_l.append(self.y[j])\n",
    "            yield torch.cat(x_l, dim=0), torch.LongTensor(y_l)\n",
    "\n",
    "test = myDataset()\n",
    "\n",
    "for train, valid in test.KFolder():\n",
    "    for x, y in test.DataIter(7, train):\n",
    "        print(x.shape)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data_iter, valid_data_iter, batch_size, num_epochs, lr, device):\n",
    "    # 初始化参数\n",
    "    def init_weight(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weight)\n",
    "    \n",
    "    print(f'training on: {device}')\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=lr)\n",
    "    loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    res = [] # 训练集梯度，训练集正确率，训练集总数\n",
    "    for epoch in range(num_epochs):\n",
    "        recorder = Accumulator(3) # 训练集梯度，训练集正确率，训练集总数\n",
    "        net.train()\n",
    "        for X, y in train_data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            yhat = net(X)\n",
    "            print(yhat.shape) \n",
    "            print(y.shape)\n",
    "            l = loss(yhat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct = (yhat.argmax(axis=1) == y).sum()\n",
    "            acc = int(correct) / int(y.shape[0])\n",
    "\n",
    "            recorder.add(float(l * X.shape[0]), correct, X.shape[0]) #说明lossfun里，梯度除以了bachsize\n",
    "\n",
    "        # 正确率和平均损失\n",
    "        net.eval()\n",
    "        validRecorder = Accumulator(3)\n",
    "        for X, y in valid_data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            yhat = net(X)\n",
    "            l = loss(yhat, y)\n",
    "            correct = (yhat.argmax(axis=1) == y).sum()\n",
    "            acc = int(correct) / int(y.shape[0])\n",
    "            validRecorder.add(float(l * X.shape[0]), correct, X.shape[0]) #说明lossfun里，梯度除以了bachsize\n",
    "\n",
    "\n",
    "        print(f'epoch = {epoch+1}')\n",
    "        print(f'训练集正确率：{recorder[1]/recorder[2]:.3f}，训练集平均loss {recorder[0]/recorder[2]:.3f}.', end=' ')\n",
    "        print(f'验证集正确率：{validRecorder[1]/validRecorder[2]:.3f}，验证集平均loss {validRecorder[0]/validRecorder[2]:.3f}.')\n",
    "        \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d: torch.Size([40, 2, 10, 49])\n",
      "ReLU: torch.Size([40, 2, 10, 49])\n",
      "Dropout: torch.Size([40, 2, 10, 49])\n",
      "MaxPool2d: torch.Size([40, 2, 5, 24])\n",
      "Conv2d: torch.Size([40, 2, 3, 10])\n",
      "Flatten: torch.Size([40, 60])\n",
      "Dropout: torch.Size([40, 60])\n",
      "Linear: torch.Size([40, 12])\n",
      "ReLU: torch.Size([40, 12])\n",
      "Dropout: torch.Size([40, 12])\n",
      "Linear: torch.Size([40, 3])\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "\n",
    "def get_net(isPCA):\n",
    "    if isPCA: # 输入变成了[1, 30, 30]\n",
    "        return torch.nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, padding=(0, 0), stride=3), # [1, 30, 30] -> [2, 12, 44]\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),   # [1, 12, 44] -> [1, 6, 22]\n",
    "        nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2),   # [1, 6, 22] -> [1, 2, 10]\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(20, 12)\n",
    "        , nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(12, 3)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # 输入变成了[1, 30, 151]\n",
    "        return torch.nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 7), padding=(0, 0), stride=3), # [1, 30, 151] -> [2, 10, 49]\n",
    "        nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),   # [2, 10, 49] -> [2, 5, 24]\n",
    "        nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(3, 6), stride=(1, 2)),   # [2, 5, 24] -> [2, 3, 10]\n",
    "        nn.Flatten(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(60, 12)\n",
    "        , nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(12, 3)\n",
    "        )\n",
    "\n",
    "x = torch.rand((40, 1, 30, 151))\n",
    "model = get_net(0)\n",
    "for layer in model.children():\n",
    "    x = layer(x)\n",
    "    print(f\"{layer.__class__.__name__}: {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cpu\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23140\\2824604127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# 正式训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     data_l = train(get_net(False), dataset.DataIter(batch_size, train_idx),\n\u001b[0m\u001b[0;32m     10\u001b[0m                     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                       batch_size, num_epoch, lr, device='cpu')\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23140\\3755743687.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_data_iter, valid_data_iter, batch_size, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3053\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "ISPCA = False\n",
    "dataset = myDataset()\n",
    "# 训练超参数\n",
    "lr = 0.05\n",
    "num_epoch = 10\n",
    "batch_size = 5\n",
    "for train_idx, valid_idx in dataset.KFolder():\n",
    "    # 正式训练\n",
    "    data_l = train(get_net(False), dataset.DataIter(batch_size, train_idx),\n",
    "                    dataset.DataIter(batch_size, valid_idx),\n",
    "                      batch_size, num_epoch, lr, device='cpu')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
